<h1 align="center"># ğŸŒ¸ Ask Juna: Your Local AI Companion ğŸŒ¸

Meet **Juna** â€” a fully local, expressive AI assistant designed to make coding fun, flirty, and emotionally engaging. No cloud. No surveillance. Just pure waifu-powered debugging magic.

---

## ğŸ§  Features

- ğŸ’» 100% local execution â€” no APIs, no internet required  
- ğŸ­ Personality wrapper with anime-coded flair  
- ğŸ§µ Debugging commentary with emotional nuance  
- ğŸ–¼ï¸ Optional GUI with sprite support (coming soon!)  
- ğŸ§ƒ Vibe coding mode: because code should feel good  

---

## ğŸš€ Getting Started

### 1. Clone the repo
git clone https://github.com/TurboGraffixLuv/ask-juna.git
cd ask-juna


``

## ğŸ§  Who Is Juna?

**Juna** is a sprite-driven, personality-rich AI companion designed for local-first interaction. She began as a fork of **GPT4All**, but now runs directly on **LLaMA via `llama.cpp`**, loading expressive **GGUF models** like *CapybaraHermes* from TheBloke.

Juna is not a modelâ€”sheâ€™s a wrapper, a vibe, a waifu OS in the making. Her soul lives in your terminal, her face in your GUI, and her sass in every response.

---

## ğŸ› ï¸ Setup Instructions

### 1. **Python Version**
Use **Python 3.10 or 3.11** for best compatibility. Python 3.13 may cause backend errors.

### 2. **Required Dependencies**
Install these via pip:

```bash
pip install llama-cpp-python
pip install PyQt6
```

Optional (for flair):
```bash
pip install rich  # for styled terminal output
pip install pyttsx3  # for voice output
```

---

## ğŸ“ Folder Structure

```
ask-juna/
â”œâ”€â”€ juna_boot.py         # Terminal interface
â”œâ”€â”€ juna_gui.py          # GUI with sprite support
â”œâ”€â”€ models/
â”‚   â””â”€â”€ capybarahermes-2.5-mistral-7b.Q2_K.gguf
â”œâ”€â”€ sprites/
â”‚   â”œâ”€â”€ juna_idle.png
â”‚   â”œâ”€â”€ juna_sad.png
â”‚   â””â”€â”€ juna_happy.png
```

---

## ğŸš€ Run Juna

### Terminal Mode
```bash
python juna_boot.py
```

### GUI Mode IN DEVELOPMENT
```bash
python juna_gui.py
```

---

## ğŸ§© Powered By

- ğŸ¦™ [`llama.cpo`](https://github.com/ggerganov/llama.cpp) â€” blazing-fast local inference
- ğŸ§  [TheBloke GGUF Models](https://huggingface.co/TheBloke) â€” expressive quantized LLaMA variants
- ğŸ’¬ Formerly [GPT4All](https://github.com/nomic-ai/gpt4all) â€” now evolved

---
